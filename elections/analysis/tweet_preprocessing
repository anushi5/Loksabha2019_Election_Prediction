import re
import sys
import pandas as pd
import textblob as tb
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import numpy as np
import hashlib
import csv
from sklearn.model_selection import train_test_split
import operator
from nltk import PorterStemmer
import os
from collections import Counter
import pickle
import importlib
import traceback
import inspect


filePath1="C:/Users/Anushi/Desktop/major2/elections-2019-master/electionsentiment/train"
print(filePath1)

'''
csfp = os.path.abspath(os.path.dirname(__file__))
if csfp not in sys.path:
    sys.path.append(csfp)
'''



def processStopWords():
    df = pd.read_csv('C:/Users/Anushi/Desktop/major2/elections-2019-master/electionsentiment/publicsentiments/stopwords.csv',engine='python')
    stop_words = set(df['Word'])
    return stop_words

stop_words = processStopWords()
ps1 = PorterStemmer()


###############################################################################
##utility code
###############################################################################

def file_to_wordset(filename):
    ''' Converts a file with a word per line to a Python set '''
    words = []
    with open(filename, 'r') as f:
        for line in f:
            words.append(line.strip())
    return set(words)


def write_status(i, total):
    ''' Writes status of a process to console '''
    sys.stdout.write('\r')
    sys.stdout.write('Processing %d/%d' % (i, total))
    sys.stdout.flush()


def save_results_to_csv(results, csv_file):
    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''
    with open(csv_file, 'w') as csv:
        csv.write('id,prediction, polarity, subjectivity, compound, neg, neu, pos, a_count, v_count, d_count, affect_count, mood, tweet\n')
        for tweet_id, pred, nlpol, nlsub, compound, neg, neu, pos , ascore, vscore, dscore, affectscore, mood, tweet in results:
            csv.write(tweet_id)
            csv.write(',')
            csv.write(str(pred))
            csv.write(',')
            csv.write(str(nlpol))
            csv.write(',')
            csv.write(str(nlsub))
            csv.write(',')
            csv.write(str(compound))
            csv.write(',')
            csv.write(str(neg))
            csv.write(',')
            csv.write(str(neu))
            csv.write(',')
            csv.write(str(pos))
            csv.write(',')

            csv.write(str(ascore))
            csv.write(',')
            csv.write(str(vscore))
            csv.write(',')
            csv.write(str(dscore))
            csv.write(',')
            csv.write(str(affectscore))
            csv.write(',')
            csv.write(str(mood))
            csv.write(',')

            csv.write(str(tweet))
            csv.write(',')
            csv.write('\n')


def top_n_words(pkl_file_name, N, shift=0):
    """
    Returns a dictionary of form {word:rank} of top N words from a pickle
    file which has a nltk FreqDist object generated by stats.py
    Args:
        pkl_file_name (str): Name of pickle file
        N (int): The number of words to get
        shift: amount to shift the rank from 0.
    Returns:
        dict: Of form {word:rank}
    """
    with open(pkl_file_name, 'rb') as pkl_file:
        freq_dist = pickle.load(pkl_file)
    most_common = freq_dist.most_common(N)
    words = {p[0]: i + shift for i, p in enumerate(most_common)}
    return words


def top_n_bigrams(pkl_file_name, N, shift=0):
    """
    Returns a dictionary of form {bigram:rank} of top N bigrams from a pickle
    file which has a Counter object generated by stats.py
    Args:
        pkl_file_name (str): Name of pickle file
        N (int): The number of bigrams to get
        shift: amount to shift the rank from 0.
    Returns:
        dict: Of form {bigram:rank}
    """
    with open(pkl_file_name, 'rb') as pkl_file:
        freq_dist = pickle.load(pkl_file)
    most_common = freq_dist.most_common(N)
    bigrams = {p[0]: i for i, p in enumerate(most_common)}
    return bigrams


def split_data(tweets, validation_split=0.1):
    """Split the data into training and validation sets
    Args:
        tweets (list): list of tuples
        validation_split (float, optional): validation split %
    Returns:
        (list, list): training-set, validation-set
    """
    index = int((1 - validation_split) * len(tweets))
    random.shuffle(tweets)
    return tweets[:index], tweets[index:]



###############################################################################


###############################################################################
##Mood processing
###############################################################################

retweet_dict = {}

filePath="C:/Users/Anushi/Desktop/major2/elections-2019-master/electionsentiment"
print (filePath)



mood_positives =['faith', 'joy']
mood_negatives = ['sadness',  'arousal', 'dominance', 'anger']
sentiments_file_path = filePath +'/publicsentiments/'

POSITIVE_WORDS_FILE = sentiments_file_path + 'positive-words.csv'
NEGATIVE_WORDS_FILE = sentiments_file_path + 'negative-words.csv'
ANGER_WORDS_FILE = sentiments_file_path + 'anger.csv'
HAPPINESS_WORDS_FILE = sentiments_file_path + 'happiness.csv'
JEALOUS_WORDS_FILE = sentiments_file_path + 'jealous.csv'
SHAME_WORDS_FILE = sentiments_file_path + 'shame.csv'
SADNESS_WORDS_FILE = sentiments_file_path + 'sadness.csv'
SUICIDAL_WORDS_FILE = sentiments_file_path + 'suicidal.csv'
FEAR_WORDS_FILE = sentiments_file_path + 'fear.csv'
SURPRISE_WORDS_FILE = sentiments_file_path + 'surprise.csv'


AROUSAL_WORDS_FILE = sentiments_file_path + 'a-scores.csv'
DOMINANCEL_WORDS_FILE = sentiments_file_path + 'd-scores.csv'
AFFECTDM_WORDS_FILE = sentiments_file_path + 'affectdm-scores.csv'
EXCITEMENT_WORDS_FILE = sentiments_file_path + 'excitement.csv'
FAITH_WORDS_FILE  = sentiments_file_path + 'faith.csv'
ALL_WORDS_FILE = sentiments_file_path + 'avd-scores.csv'
SUPPORT_PHRASES = sentiments_file_path + 'support.csv'

positive_words = pd.read_csv(POSITIVE_WORDS_FILE)['Word']
negative_words = pd.read_csv(NEGATIVE_WORDS_FILE)['Word']
all_words = pd.read_csv(ALL_WORDS_FILE)['Word'].str.lower()
v_score = pd.read_csv(ALL_WORDS_FILE)['Valence']
a_score = pd.read_csv(ALL_WORDS_FILE)['Arousal']
d_score = pd.read_csv(ALL_WORDS_FILE)['Dominance']
affect_words  = pd.read_csv(AFFECTDM_WORDS_FILE)['term'].str.lower()
affect_mood = pd.read_csv(AFFECTDM_WORDS_FILE)['AffectDimension'].str.lower().values

shame_words= pd.read_csv(SHAME_WORDS_FILE)['Word'].str.lower()
anger_words = pd.read_csv(ANGER_WORDS_FILE)['Word'].str.lower()
sadness_words = pd.read_csv(SADNESS_WORDS_FILE)['Word'].str.lower()
suicidal_words = pd.read_csv(SUICIDAL_WORDS_FILE)['Word'].str.lower()
happiness_words = pd.read_csv(HAPPINESS_WORDS_FILE)['Word'].str.lower()
jealous_words = pd.read_csv(JEALOUS_WORDS_FILE)['Word'].str.lower()
surprise_words = pd.read_csv(SURPRISE_WORDS_FILE)['Word'].str.lower()
faith_words = pd.read_csv(FAITH_WORDS_FILE)['Word'].str.lower()

support_phrases =  pd.read_csv(SUPPORT_PHRASES)['Word'].str.lower()


def get_n_gram_mood(n_gram_word):

    all_moods = []
    v_count, a_count, d_count, affect_count = 0, 0, 0, 0
    pos_count, neg_count = 0, 0
    sid = SentimentIntensityAnalyzer()
    total_ss = 0
    max_len = 0

    for word in n_gram_word:
        #stem_word = ps.stem(word)
        blob = tb.TextBlob(word)

        context_mood = False
        max_len = max_len + 1
        ss = sid.polarity_scores(word)
        total_ss = ss['compound'] + total_ss

        if len(affect_words[word == affect_words]) > 0:
            affect_count += 1
            itemIndex = (affect_words[word == affect_words]).index[0]
            all_moods.append(affect_mood[itemIndex])
            context_mood = True
        elif len(all_words[word == all_words]) > 0:
            a_count = float(a_score[all_words[word == all_words].index[0]])
            v_count = float(v_score[all_words[word == all_words].index[0]])
            d_count = float(d_score[all_words[word == all_words].index[0]])

        if (len(jealous_words[(jealous_words == word)])):
            all_moods.append('anger')
            context_mood = True
        elif (len(shame_words[(shame_words == word)])):
            all_moods.append('sadness')
            context_mood = True
        elif (len(anger_words[(anger_words == word)])):
            all_moods.append('anger')
            context_mood = True
        elif (len(sadness_words[(sadness_words == word)])):
            all_moods.append('sadness')
            context_mood = True
        elif (len(happiness_words[(happiness_words == word)])):
            all_moods.append('joy')
            context_mood = True
        elif (len(suicidal_words[(suicidal_words == word)])):
            all_moods.append('sadness')
            context_mood = True
        elif (len(suicidal_words[(surprise_words == word)])):
            all_moods.append('joy')
            context_mood = True
        elif (len(faith_words[(faith_words == word)])):
            all_moods.append('faith')
            context_mood = True
        elif (a_count > v_count and a_count > d_count and a_count > 0.50):
            all_moods.append('arousal')
            context_mood = True
        elif (d_count > a_count and d_count > v_count and d_count > 0.50):
            all_moods.append('dominance')
            context_mood = True
        elif (v_count > a_count and v_count > d_count and v_count > 0.50):
             word_polarity = blob.polarity
             if(word_polarity > 0):
                all_moods.append('joy')
             elif(word_polarity < 0):
                 all_moods.append('sadness')
             else:
                 all_moods.append('neutral')
             context_mood = True
        else:
            supportVals = support_phrases.values
            for i in range(len(supportVals)):
                if (supportVals[i] in n_gram_word):
                    all_moods.append('faith')

        if (context_mood == False):
            if word in positive_words.values:
                pos_count += 1
                all_moods.append('joy')
            elif word in negative_words.values:
                neg_count += 1
                all_moods.append('sadness')

    final_ss = float(total_ss/max_len)
    final_mood = determine_mood_combination(all_moods,final_ss, word)
    return final_mood



def determine_priority(mood_list, more_moods, summary):

    final_mood = 'dominance'
    if(len(more_moods) > 0):
        final_mood = more_moods[0]
        return final_mood

    if(summary > 0 ):
        for i in range(0, len(mood_positives)):
            if(mood_positives[i] in mood_list):
                final_mood = mood_positives[i]
                break
    else:
        for i in range(0, len(mood_negatives)):
            if(mood_negatives[i] in mood_list):
                final_mood = mood_negatives[i]
                break

    return final_mood
          

def calculate_max_mood(mood_list, summary, bDist):
    no_positives = 0
    no_negatives = 0
    mood_freq_dist = {}

    for i in range(len(mood_list)):
        if (mood_list[i] in mood_positives):
            no_positives = no_positives + 1
        elif (mood_list[i] in mood_negatives):
            no_negatives = no_negatives + 1
        if (mood_list[i] in mood_freq_dist):
            mood_freq_dist[mood_list[i]] = mood_freq_dist[mood_list[i]] + 1
        else:
            mood_freq_dist[mood_list[i]] = 0

    max_mood_item = max(mood_freq_dist.items(), key=operator.itemgetter(1))[0]

    if(bDist == True):
        if (no_positives > no_negatives and max_mood_item in mood_negatives):
            if (summary > 0):
                max_mood_item = 'joy'
        elif (no_negatives > no_positives and max_mood_item in mood_positives):
            if (summary < 0):
                max_mood_item = 'sadness'
    return max_mood_item




def determine_mood_combination(mood_list, summary, full_word):
    max_mood_item = ''

    if (len(mood_list) == 0):
        if(summary > 0):
            max_mood_item = 'joy'
        elif(summary < 0):
            max_mood_item = 'sadness'
        else:
            #print(full_word, mood_list, summary)
            max_mood_item = 'neutral'
    else:
        max_mood_item = calculate_max_mood(mood_list, summary, True)

    return max_mood_item



def search_context_word(word, tweet):
    all_moods = []
    affect_count = 0
    v_count, a_count, d_count = 0, 0, 0
    context_mood = False

    if len(affect_words[word == affect_words]) > 0:
        affect_count += 1
        # all_moods.append(affect_mood[affect_words[word == affect_words].index[0]])
        itemIndex = (affect_words[word == affect_words]).index[0]
        all_moods.append(affect_mood[itemIndex])
        context_mood = True
    elif len(all_words[word == all_words]) > 0:
        a_count = float(a_score[all_words[word == all_words].index[0]])
        v_count = float(v_score[all_words[word == all_words].index[0]])
        d_count = float(d_score[all_words[word == all_words].index[0]])

    if (len(jealous_words[(jealous_words == word)])):
        all_moods.append('anger')
        context_mood = True
    elif (len(shame_words[(shame_words == word)])):
        all_moods.append('sadness')
        context_mood = True
    elif (len(anger_words[(anger_words == word)])):
        all_moods.append('anger')
        context_mood = True
    elif (len(sadness_words[(sadness_words == word)])):
        all_moods.append('sadness')
        context_mood = True
    elif (len(happiness_words[(happiness_words == word)])):
        all_moods.append('joy')
        context_mood = True
    elif (len(suicidal_words[(suicidal_words == word)])):
        all_moods.append('sadness')
        context_mood = True
    elif (len(suicidal_words[(surprise_words == word)])):
        all_moods.append('joy')
        context_mood = True
    elif (len(faith_words[(faith_words == word)])):
        all_moods.append('faith')
        context_mood = True
    elif (a_count > v_count and a_count > d_count and a_count > 0.50):
        all_moods.append('arousal')
        context_mood = True
    elif (d_count > a_count and d_count > v_count and d_count > 0.50):
        all_moods.append('dominance')
        context_mood = True
        # elif (v_count > a_count and v_count > d_count and v_count > 0.50):
        # all_moods.append('joy')
    else:
        supportVals = support_phrases.values
        for i in range(len(supportVals)):
            if (supportVals[i] in tweet):
                all_moods.append('faith')

    return all_moods, context_mood, a_count, v_count, d_count, affect_count

def determine_vocab_mood(word, tweet, ss):
    # word = ps.stem(full_word)
    word_mood = []
    pos_count, neg_count = 0, 0
    word_mood, context_mood, a_count, v_count, d_count, affect_count = search_context_word(word,tweet)

    if (context_mood == False):
        word_mood, context_mood, a_count, v_count, d_count, affect_count = search_context_word(ps.stem(word), tweet)
        if (context_mood == False):
            if word in positive_words.values:
                pos_count += 1
                word_mood.append('joy')
            elif word in negative_words.values:
                neg_count += 1
                word_mood.append('sadness')


    return word_mood, pos_count, neg_count


def classifyLabelMoods(processing_results):

    predictions =[]
    for i in range(0, len(processing_results)):
        all_moods = []
        pos_count, neg_count = 0, 0
        [tweet_id, tweet, creation_date, favourites_count, statuses_count, followers_count, retweeted, retweet_count,
         processed_retweet, location, hashtags, user_mentions, symbols, urls] = processing_results[i]

        tweetHash = (hashlib.md5(tweet.encode('utf-8'))).hexdigest()
        if (tweetHash not in retweet_dict.keys()):
            retweet_dict[tweetHash] = retweet_count
            try:
                tweetIdType = type(int(tweet_id))
            except ValueError:
                tweet_id = "1234"
                continue

            sid = SentimentIntensityAnalyzer()
            ss = sid.polarity_scores(tweet)
            procList = []
            procList.append(ss['neg'])
            procList.append(ss['neu'])
            procList.append(ss['pos'])
            procList.append(ss['compound'])

            for word in tweet.split():
                word_mood, pos_count, neg_count = determine_vocab_mood(word, tweet, ss)
                if(len(word_mood) > 0):
                    all_moods.append(word_mood[0])

            final_mood = determine_mood_combination(all_moods, ss['compound'], word)
            print(final_mood,all_moods, tweet)


            blob = tb.TextBlob(tweet)

            if pos_count >= neg_count:
                prediction = 1
            elif blob.sentiment.polarity < 0:
                prediction = -1
            else:
                prediction = 0

            predictions.append((tweet_id, prediction, "{0:.2f}".format(blob.sentiment.polarity),
                                    "{0:.2f}".format(blob.sentiment.subjectivity), ss['compound'], ss['neg'], ss['neu'],
                                    ss['pos'], final_mood, tweet, creation_date, favourites_count, statuses_count, followers_count, retweeted, retweet_count,
                                processed_retweet, location, hashtags, user_mentions, symbols, urls))

    return predictions

def dump_to_csv(results, csv_file):
    moodCsvWriter = csv.writer(open(csv_file, 'w'), delimiter=',')
    moodCsvWriter.writerow(['id', 'prediction', 'polarity', 'subjectivity', 'compound', 'neg', 'neu', 'pos', 'mood', 'tweet', 'created_at', 'favourites_count', 'statuses_count', 'followers_count', 'retweeted', 'retweet_count', 'retweeted_text', 'location', 'hashtags', 'user_mentions', 'symbols', 'urls'])
    for i in range(0, len(results)):
        moodCsvWriter.writerow(list(results[i]))

    lastInd = csv_file.rfind('/')
    train_file_name = csv_file[0: lastInd] + "/train-dataset/" + csv_file[lastInd+1:len(csv_file)-4] + "-train.csv"
    test_file_name = csv_file[0: lastInd] + "/test-dataset/" +  csv_file[lastInd+1:len(csv_file) - 4] + "-test.csv"

    dataset = pd.read_csv(filePath +'/train/train/sentiments/LokShobaElc2019BJP-moods.csv')

    train, test = train_test_split(dataset[:-1], test_size=0.2)

    train.to_csv(train_file_name, index=False, index_label=False, columns=['id', 'prediction', 'polarity', 'subjectivity', 'compound', 'neg', 'neu', 'pos', 'mood', 'tweet', 'created_at', 'favourites_count', 'statuses_count', 'followers_count', 'retweeted', 'retweet_count', 'retweeted_text', 'location', 'hashtags', 'user_mentions', 'symbols', 'urls'])
    test.to_csv(test_file_name, index=False, index_label=False, columns=['id', 'prediction', 'polarity', 'subjectivity', 'compound', 'neg', 'neu', 'pos', 'mood', 'tweet', 'created_at', 'favourites_count', 'statuses_count', 'followers_count', 'retweeted', 'retweet_count', 'retweeted_text', 'location', 'hashtags', 'user_mentions', 'symbols', 'urls'])



def processMoods(processing_results, mood_file_name):
    print(mood_file_name, len(processing_results))
    predictions = classifyLabelMoods(processing_results)
    dump_to_csv(predictions, mood_file_name)



###############################################################################
##tweet processing
###############################################################################


def preprocess_word(word):
    # Remove punctuation
    word = word.strip('\'"?!,.():;..')
    # Convert more than 2 letter repetitions to 2 letter
    word = re.sub(r'(.)\1+', r'\1\1', word)
    # Remove - & '
    word = re.sub(r'(-|\')', '', word)
   # # Remove 2 or more dots
    #word = re.sub(r'(..)', '', word)
    return word


def is_valid_word(word):
    # Check if word begins with an alphabet
    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\._][^0-9]*$', word) is not None)




def preprocess_tweet(tweet):
    processed_tweet = []
   ## emoji = []
    # Convert to lower case
    if(tweet != None):
        tweet = tweet.lower()
        # Replaces URLs with the word URL
        tweet = re.sub(r'((www\.[\S]+)|(https?://[\S]+))', ' URL ', tweet)
        # Replace @handle with the word USER_MENTION
        tweet = re.sub(r'@[\S]+', '', tweet)
        # Replaces #hashtag with hashtag
        tweet = re.sub(r'#(\S+)', r' \1 ', tweet)
        # Remove RT (retweet)
        tweet = re.sub(r'\brt\b', '', tweet)
        # Replace 2+ dots with space
        tweet = re.sub(r'\.{2,}', ' ', tweet)
        # Replace ,  with space
        tweet = re.sub(r'\,', ' ', tweet)
        # Strip space, " and ' from tweet
        tweet = tweet.strip(' "\'')
        # Replace emojis with either EMO_POS or EMO_NEG
     ##   emoji = handle_emojis(tweet)
        #if(emoji != ''):
            #print("Mood is " + emoji)
        # Replace multiple spaces with a single space
        tweet = re.sub(r'\s+', ' ', tweet)
        words = tweet.split()

        for word in words:
            word = preprocess_word(word)
            if is_valid_word(word) and word not in stop_words:
                if ('b' not in word):
                    processed_tweet.append(word)
                    #word = str(porter_stemmer.stem(word))

        return ' '.join(processed_tweet)

def get_ngram_freqdist(ngrams):
    freq_dict = {}
    for ngram in ngrams:
        if(ngram in freq_dict):
            freq_dict[ngram] += 1
        else:
            freq_dict[ngram] = 1
    counter = Counter(freq_dict)
    return counter


def get_ngrams(tweet_words, n):
    ngrams = []
    num_words = len(tweet_words)
    for i in range(num_words -(n-1)):
        lookUpTweets = []

        for j in range(i, i+n):
            lookUpTweets.append(tweet_words[j])

        ngrams.append(tuple(lookUpTweets))

    return ngrams


def get_ngrams_all(tweet_words, n):
    ngrams = []
    num_words = len(tweet_words)
    for i in range(num_words -(n-1)):
        ngrams.append((tweet_words[i], tweet_words[i + 1], tweet_words[i + 2], tweet_words[i + 3]))
    return ngrams

def get_bigrams(tweet_words):
    bigrams = []
    num_words = len(tweet_words)
    for i in range(num_words - 1):
        bigrams.append((tweet_words[i], tweet_words[i + 1]))
    return bigrams

def preprocess_csv(csv_file_name, processed_file_name):
   ## emoji = []
    save_to_file = open(processed_file_name, 'w')

    df = pd.read_csv(csv_file_name)
    tweetText = df['full_text']
    retweetText = df['retweeted_text']
    tweet_id = df['id_str']
    save_to_file.write('%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\n' % ('id_str', 'full_text', 'created_at', 'favourites_count', 'statuses_count', 'followers_count', 'retweeted', 'retweet_count', 'retweeted_text', 'location', 'hashtags', 'user_mentions', 'symbols', 'urls'))
    processed_tweet = ''
    processed_retweet = ''
    results = []

    for i in range(0, len(tweetText)):

        if (pd.isnull(tweetText[i]) == False):
            processed_tweet = preprocess_tweet(tweetText[i])

        if(pd.isnull(retweetText[i]) == False):
            processed_retweet = preprocess_tweet(retweetText[i])

        save_to_file.write('%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\n' % (tweet_id[i], processed_tweet,df['created_at'][i],df['favourites_count'][i],df['statuses_count'][i],df['followers_count'][i],df['retweeted'][i],df['retweet_count'][i],processed_retweet,df['location'][i], df['hashtags'][i], df['user_mentions'][i], df['symbols'][i], df['urls'][i]))
        results.append([tweet_id[i], processed_tweet,df['created_at'][i],df['favourites_count'][i],df['statuses_count'][i],df['followers_count'][i],df['retweeted'][i],df['retweet_count'][i],processed_retweet,df['location'][i], df['hashtags'][i], df['user_mentions'][i], df['symbols'][i], df['urls'][i]])

    save_to_file.close()
    print ('\nSaved processed tweets to: %s' % processed_file_name)
    return results


def evaluate_mood_n_grams():
    return

def analyze_tweet(tweet, user_mentions, urls, hash_tags):
    result = {}
    result['MENTIONS'] = user_mentions
    result['URLS'] = urls
    result['HASHTAGS'] = hash_tags
    words = tweet.split()

    result['WORDS'] = len(words)
    bigrams = get_ngrams(words,2)
    result['BIGRAMS'] = len(bigrams)

    trigrams = get_ngrams(words, 3)
    result['TRIGRAMS'] = len(trigrams)

    quadgrams = get_ngrams(words,4)
    result['QUADGRAMS'] = len(quadgrams)

    return result, words, bigrams, trigrams, quadgrams


def getTweetStats(preprocessed_file_name, ipkl_file_name):

    num_tweets, num_pos_tweets, num_neg_tweets = 0, 0, 0
    num_mentions, max_mentions, max_hashtags = 0, 0, 0
    num_urls, max_urls , num_hashtags = 0, 0, 0
    num_words, num_unique_words, min_words, max_words = 0, 0, 1e6, 0
    num_bigrams, num_unique_bigrams = 0, 0
    num_trigrams, num_unique_trigrams = 0, 0
    num_quadgrams, num_unique_quadgrams = 0, 0

    all_words = []
    all_bigrams = []
    all_trigrams = []
    all_quadgrams = []
    all_tri_moods = []
    all_fi_moods = []

    with open(preprocessed_file_name, 'r') as csv:
        lines = csv.readlines()
        for i, line in enumerate(lines):
            tweet = ''
            colCount = len(line.strip().split(','))
            if(colCount == 14):
                t_id, tweet, created_at, favourites_count, statuses_count, followers_count, retweeted, retweet_count, retweeted_text, location, hastags, user_mentions, symbols, urls = line.strip().split(',')
            if(i > 0):
                try:
                    result, words, bigrams, trigrams, quadgrams = analyze_tweet(tweet, user_mentions, urls, hastags)
                    if(result['MENTIONS'] != 'user_mentions'):
                        num_mentions += int(float(result['MENTIONS']))
                    else:
                        num_mentions = 0

                    max_mentions = max(max_mentions, num_mentions)
                    num_urls += int(float(result['URLS']))
                    max_urls = max(max_urls, int(float(result['URLS'])))
                    num_hashtags += int(float(result['HASHTAGS']))
                    max_hashtags = max(max_hashtags, int(float(result['HASHTAGS'])))
                    num_words += int(float(result['WORDS']))
                    min_words = min(min_words, int(float(result['WORDS'])))
                    max_words = max(max_words, int(float(result['WORDS'])))
                    all_words.extend(words)
                    num_bigrams += result['BIGRAMS']
                    num_trigrams += result['TRIGRAMS']
                    num_quadgrams += result['QUADGRAMS']
                    all_bigrams.extend(bigrams)
                    all_trigrams.extend(trigrams)
                    all_quadgrams.extend(quadgrams)
                    num_tweets = num_tweets + 1

                except Exception as e:
                    print("type error: " + str(e))
                    print(traceback.format_exc())

        unique_words = list(set(all_words))
        with open(ipkl_file_name + '-unique.txt', 'w') as uwf:
            uwf.write('\n'.join(unique_words))
        num_unique_words = len(unique_words)
        num_unique_bigrams = len(set(all_bigrams))
        num_unique_trigrams = len(set(all_trigrams))
        num_unique_quadgrams = len(set(all_quadgrams))

        print('\nCalculating frequency distribution')

        # Unigrams
        uni_freq_dist = get_ngram_freqdist(all_words)
        pkl_file_name = ipkl_file_name + '-freqdist.pkl'
        lind = pkl_file_name.rfind('wordstats/')
        maxlen = lind+len('wordstats/')
        pkl_file_name_wdir = pkl_file_name[:maxlen] + "1-gram/" + pkl_file_name[maxlen : len(pkl_file_name)]

        uni_csv_file_name = ipkl_file_name + '-freqdist-uni.csv'
        csv_file_name_wdir = uni_csv_file_name[:maxlen] + "1-gram/" + uni_csv_file_name[maxlen: len(uni_csv_file_name)]

        columns = ['W1', 'F']
        df = pd.DataFrame(columns=columns)
        cnt = 0
        for unigram in uni_freq_dist:
            uList = []
            key = unigram
            uvalue = uni_freq_dist[key]
            uList.append(key)
            uList.append(uvalue)
            df.loc[cnt] = uList
            cnt = cnt + 1
        sorteddf = df.sort_values(by=['F'], ascending=False)
        sorteddf.to_csv(csv_file_name_wdir, index=False)

        # Bigrams
        bigram_freq_dist = get_ngram_freqdist(all_bigrams)
        bi_pkl_file_name = ipkl_file_name + '-freqdist-bi.pkl'
        lind = bi_pkl_file_name.rfind('wordstats/')
        maxlen = lind + len('wordstats/')
        pkl_file_name_wdir = bi_pkl_file_name[:maxlen] + "2-gram/" + bi_pkl_file_name[maxlen: len(bi_pkl_file_name)]
        bi_csv_file_name = ipkl_file_name + '-freqdist-bi.csv'
        csv_file_name_wdir = bi_csv_file_name[:maxlen] + "2-gram/" + bi_csv_file_name[maxlen: len(bi_csv_file_name)]


        columns = ['W1', 'W2', 'F']
        df = pd.DataFrame(columns=columns)
        cnt = 0
        for bgram in bigram_freq_dist:
            bList = []
            key = bgram
            bvalue = bigram_freq_dist[key]
            bList =list(key)
            bList.append(bvalue)
            df.loc[cnt] = bList
            cnt = cnt +1
        sorteddf = df.sort_values(by = ['F'], ascending=False)
        sorteddf.to_csv(csv_file_name_wdir, index = False)

        # Trigrams
        tri_pkl_file_name = ipkl_file_name + '-freqdist-tri.pkl'
        trigram_freq_dist = get_ngram_freqdist(all_trigrams)
        lind = tri_pkl_file_name.rfind('wordstats/')
        maxlen = lind + len('wordstats/')
        pkl_file_name_wdir = tri_pkl_file_name[:maxlen] + "3-gram/" + tri_pkl_file_name[maxlen: len(tri_pkl_file_name)]

        tri_csv_file_name = ipkl_file_name + '-freqdist-tri.csv'
        csv_file_name_wdir = tri_csv_file_name[:maxlen] + "3-gram/" + tri_csv_file_name[maxlen: len(tri_csv_file_name)]

        columns = ['W1', 'W2', 'W3','F']
        df = pd.DataFrame(columns=columns)
        cnt = 0

        try:
            for trigram in trigram_freq_dist:
                triList = []
                key = trigram
                trivalue = trigram_freq_dist[key]
                triList = list(key)
                triList.append(trivalue)
                df.loc[cnt] = triList
                cnt = cnt + 1

        except Exception as e:
            print(traceback.format_exc())

        sorteddf = df.sort_values(by=['F'], ascending=False)
        sorteddf.to_csv(csv_file_name_wdir, index=False)

        fi_csv_file_name = ipkl_file_name + '-freqdist-quad.csv'
        csv_file_name_wdir = fi_csv_file_name[:maxlen] + "4-gram/" + fi_csv_file_name[maxlen: len(fi_csv_file_name)]

        quad_pkl_file_name = ipkl_file_name + '-freqdist-quad.pkl'
        quadgram_freq_dist = get_ngram_freqdist(all_quadgrams)
        lind = quad_pkl_file_name.rfind('wordstats/')
        maxlen = lind + len('wordstats/')
        pkl_file_name_wdir = quad_pkl_file_name[:maxlen] + "4-gram/" + quad_pkl_file_name[maxlen: len(quad_pkl_file_name)]

        columns = ['W1', 'W2', 'W3', 'W4' ,'F']
        df = pd.DataFrame(columns=columns)
        cnt = 0
        for figram in quadgram_freq_dist:

            key = figram
            fivalue = quadgram_freq_dist[key]
            fiList = list(key)
            fiList.append(fivalue)
            df.loc[cnt] = fiList
            cnt = cnt + 1

        sorteddf = df.sort_values(by=['F'], ascending=False)
        sorteddf.to_csv(csv_file_name_wdir, index=False)


        print('Saved bi-frequency distribution to %s' % bi_pkl_file_name)
        print('Saved tri-frequency distribution to %s' % tri_pkl_file_name)
        print('\n[Analysis Statistics]')
        print('Tweets => Total: %d, Positive: %d, Negative: %d' % (num_tweets, num_pos_tweets, num_neg_tweets))
        print('User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions))
        print('URLs => Total: %d, Avg: %.4f, Max: %d' % (num_urls, num_urls / float(num_tweets), max_urls))
        print('HASHTAGS => Total: %d, Avg: %.4f, Max: %d' % (num_hashtags, num_hashtags / float(num_tweets), max_hashtags))

        #print('Emojis => Total: %d, Positive: %d, Negative: %d, Avg: %.4f, Max: %d' % (num_emojis, num_pos_emojis, num_neg_emojis, num_emojis / float(num_tweets), max_emojis))
        print('Words => Total: %d, Unique: %d, Avg: %.4f, Max: %d, Min: %d' % (num_words, num_unique_words, num_words / float(num_tweets), max_words, min_words))
        print('Bigrams => Total: %d, Unique: %d, Avg: %.4f' % (num_bigrams, num_unique_bigrams, num_bigrams / float(num_tweets)))
        print('Trigrams => Total: %d, Unique: %d, Avg: %.4f' % (num_trigrams, num_unique_trigrams, num_trigrams / float(num_tweets)))
        print('Quadgrams => Total: %d, Unique: %d, Avg: %.4f' % (num_quadgrams, num_unique_quadgrams, num_quadgrams / float(num_tweets)))


if __name__ == '__main__':

    path1 = filePath1 + "/train/"
    
    print(filePath1)
    print (path1)
    
    csv_file_names = os.listdir(path1 + "raw/")


    for i in range(0, len(csv_file_names)):
        if('csv' in csv_file_names[i]):
            preprocessed_file_name =  path1  + "preprocessed/" + csv_file_names[i][:-4] + ".csv"
            mood_file_name =  path1  + "sentiments/" + csv_file_names[i][:-4] + "-moods.csv"
            processing_results  = preprocess_csv(path1 + "raw/" + csv_file_names[i], preprocessed_file_name).encode("utf-8")
            pkl_file_name = path1 + "wordstats/" + csv_file_names[i][:-4]
            getTweetStats(preprocessed_file_name, pkl_file_name)
            md.processMoods(processing_results, mood_file_name)




